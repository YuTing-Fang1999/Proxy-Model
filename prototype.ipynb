{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Operations\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Reading/Writing Data\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# For Progress Bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Pytorch\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import qmc\n",
    "from itertools import product"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "config = {\n",
    "    'seed': 6302,      # Your seed number, you can pick your lucky number. :)\n",
    "    \"gen_num\": 100, \n",
    "    'valid_ratio': 0.2,   # validation_size = train_size * valid_ratio\n",
    "    'n_epochs': 6000,     # Number of epochs.            \n",
    "    'batch_size': 16, \n",
    "    'learning_rate': 1e-4,              \n",
    "    'early_stop': 800,    # If model has not improved for this many consecutive epochs, stop training.     \n",
    "    'save_path': './models/model.ckpt'  # Your model will be saved here.\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definition \n",
    "模擬影像處理的流程  \n",
    "x: 原圖  \n",
    "p: 所下的參數  \n",
    "y: 處理過的圖 \n",
    "\n",
    "### $y = f(x, p)$\n",
    "假設x只有兩個pixel，且這兩個pixel會互相影響  \n",
    "$y_0 = p_0 * x_0^{2} + p_1 * x_1 + p_2$  \n",
    "$y_1 = p_0 * x_1^{2} + p_1 * x_0 + x_1$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, p): # x,p -> y\n",
    "    x = np.array(x).T\n",
    "    p = np.array(p).T\n",
    "    y0 = p[0]*x[0]**2 + p[1]*x[1] + p[2]\n",
    "    y1 = p[0]*x[1]**2 + p[1]*x[0] + x[1]\n",
    "    return np.array([y0, y1]).T\n",
    "\n",
    "# ex\n",
    "# x = [[1, 2]]\n",
    "# p = [[0.1, 0.2, 0.3], [0.2, 0.2, 0.3], [0.3, 0.2, 0.3]] # parameters are normalized to [0, 1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    '''\n",
    "    x: Features.\n",
    "    y: Targets, if none, do prediction.\n",
    "    '''\n",
    "    def __init__(self, x, y, p):\n",
    "        self.x = torch.FloatTensor(x)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "        self.p = torch.FloatTensor(p)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "            return self.x[idx], self.y[idx], self.p[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "def same_seed(seed): \n",
    "    '''Fixes random number generator seeds for reproducibility.'''\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def train_valid_split(data_set, valid_ratio, seed):\n",
    "    '''Split provided training data into training set and validation set'''\n",
    "    valid_set_size = int(valid_ratio * len(data_set)) \n",
    "    train_set_size = len(data_set) - valid_set_size\n",
    "    train_set, valid_set = random_split(data_set, [train_set_size, valid_set_size], generator=torch.Generator().manual_seed(seed))\n",
    "    return train_set, valid_set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Dataset\n",
    "* 初步實驗 假設都是同一張圖 (x固定)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[1, 2]]*config['gen_num'] # 假設圖固定，只修改參數\n",
    "sampler = qmc.LatinHypercube(d=3) # 假設p有3個參數\n",
    "p = sampler.random(n=config['gen_num']) # 隨機產生 gen_num 組\n",
    "\n",
    "dataset = MyDataset(x, f(x, p), p)\n",
    "train_dataset, valid_dataset = train_valid_split(dataset, config['valid_ratio'], config['seed'])\n",
    "\n",
    "# Pytorch data loader loads pytorch dataset into batches.\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Proxy_Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Proxy_Model, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, p):\n",
    "        # 因為目前假設都是同一張圖，因此輸入p即可\n",
    "        y = self.layers(p)\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1: Train Proxy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6000]: Train loss: 13.644409, Valid loss: 15.039849\n",
      "Epoch [501/6000]: Train loss: 0.514352, Valid loss: 0.623120\n",
      "Epoch [1001/6000]: Train loss: 0.265467, Valid loss: 0.249818\n",
      "Epoch [1501/6000]: Train loss: 0.059927, Valid loss: 0.066225\n",
      "Epoch [2001/6000]: Train loss: 0.002909, Valid loss: 0.005071\n",
      "Epoch [2501/6000]: Train loss: 0.000027, Valid loss: 0.000062\n",
      "Epoch [3001/6000]: Train loss: 0.000003, Valid loss: 0.000018\n",
      "Epoch [3501/6000]: Train loss: 0.000000, Valid loss: 0.000007\n",
      "Epoch [4001/6000]: Train loss: 0.000000, Valid loss: 0.000006\n",
      "\n",
      "Model is not improving, so we halt the training session.\n"
     ]
    }
   ],
   "source": [
    "def trainer(train_loader, valid_loader, model, config, device):\n",
    "\n",
    "    criterion = nn.MSELoss(reduction='mean') # Define your loss function, do not modify this.\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate']) \n",
    "\n",
    "    if not os.path.isdir('./models'):\n",
    "        os.mkdir('./models') # Create directory of saving models.\n",
    "\n",
    "    n_epochs, best_loss, early_stop_count = config['n_epochs'], math.inf, 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train() # Set your model to train mode.\n",
    "        loss_record = []\n",
    "\n",
    "        for x, y, p in train_loader:\n",
    "            optimizer.zero_grad()               # Set gradient to zero.\n",
    "            x, y, p = x.to(device), y.to(device), p.to(device)   # Move your data to device. \n",
    "            pred = model(x, p)             \n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()                     # Compute gradient(backpropagation).\n",
    "            optimizer.step()                    # Update parameters.\n",
    "            loss_record.append(loss.detach().item())\n",
    "\n",
    "        mean_train_loss = np.round(sum(loss_record)/len(loss_record), 6)\n",
    "\n",
    "        model.eval() # Set your model to evaluation mode.\n",
    "        loss_record = []\n",
    "        for x, y, p in valid_loader:\n",
    "            x, y, p = x.to(device), y.to(device), p.to(device)\n",
    "            with torch.no_grad():\n",
    "                pred = model(x, p)\n",
    "                loss = criterion(pred, y)\n",
    "\n",
    "            loss_record.append(loss.item())\n",
    "            \n",
    "        mean_valid_loss = np.round(sum(loss_record)/len(loss_record), 6)\n",
    "        if epoch%500==0:\n",
    "            print(f'Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.6f}, Valid loss: {mean_valid_loss:.6f}')\n",
    "\n",
    "        if mean_valid_loss < best_loss:\n",
    "            best_loss = mean_valid_loss\n",
    "            torch.save(model.state_dict(), config['save_path']) # Save your best model\n",
    "            # print('Saving model with loss {:.3f}...'.format(best_loss))\n",
    "            early_stop_count = 0\n",
    "        else: \n",
    "            early_stop_count += 1\n",
    "\n",
    "        if early_stop_count >= config['early_stop']:\n",
    "            print('\\nModel is not improving, so we halt the training session.')\n",
    "            return\n",
    "        \n",
    "proxy_model = Proxy_Model(input_dim=3, output_dim=2).to(device) # put your model and data on the same computation device.\n",
    "trainer(train_loader, valid_loader, proxy_model, config, device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step2: Fix Proxy Model and Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tune_Proxy_Model(nn.Module):\n",
    "    def __init__(self, input_dim, proxy_model):\n",
    "        super(Tune_Proxy_Model, self).__init__()\n",
    "        self.proxy_model = proxy_model\n",
    "        self.tuning_fn = nn.Linear(input_dim, input_dim, bias=False)\n",
    "\n",
    "    def forward(self, x, p):\n",
    "        # 因為目前假設都是同一張圖，因此輸入p即可\n",
    "        p = self.tuning_fn(p) # tune完後的參數\n",
    "        y = self.proxy_model(x, p)\n",
    "        return y\n",
    "    \n",
    "# Load Proxy Model\n",
    "proxy_model = Proxy_Model(input_dim=3, output_dim=2).to(device)\n",
    "proxy_model.load_state_dict(torch.load(config['save_path']))\n",
    "\n",
    "tune_model = Tune_Proxy_Model(3, proxy_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning(model, config, device, x, y):\n",
    "    x, y = torch.FloatTensor(x).to(device), torch.FloatTensor(y).to(device) # Move your data to device. \n",
    "    p = torch.FloatTensor([1, 1, 1] ).to(device)  # init p\n",
    "\n",
    "    criterion = nn.MSELoss(reduction='mean') # Define your loss function, do not modify this.\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate']) \n",
    "\n",
    "    n_epochs, best_loss, early_stop_count = 10000, math.inf, 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.tuning_fn.train()\n",
    "        model.proxy_model.eval() \n",
    "        # fix proxy model\n",
    "        for param in model.proxy_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        loss_record = []\n",
    "\n",
    "        optimizer.zero_grad()               # Set gradient to zero.\n",
    "        pred = model(x, p)             \n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()                     # Compute gradient(backpropagation).\n",
    "        optimizer.step()                    # Update parameters.\n",
    "        model.tuning_fn.weight.data.clamp_(min=0, max=1) # clip weight to [0, 1]\n",
    "        loss_record.append(loss.detach().item())\n",
    "\n",
    "        mean_train_loss = np.round(sum(loss_record)/len(loss_record), 6)\n",
    "        \n",
    "        if epoch%500==0:\n",
    "            print(f'Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.4f}')\n",
    "\n",
    "        if mean_train_loss < best_loss:\n",
    "            best_loss = mean_train_loss\n",
    "            early_stop_count = 0\n",
    "        else: \n",
    "            early_stop_count += 1\n",
    "\n",
    "        if early_stop_count >= config['early_stop']:\n",
    "            print('\\nModel is not improving, so we halt the training session.')\n",
    "            break\n",
    "        \n",
    "    return model.tuning_fn(p).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10000]: Train loss: 1.2717\n",
      "Epoch [501/10000]: Train loss: 0.2854\n",
      "Epoch [1001/10000]: Train loss: 0.1010\n",
      "Epoch [1501/10000]: Train loss: 0.0389\n",
      "Epoch [2001/10000]: Train loss: 0.0188\n",
      "Epoch [2501/10000]: Train loss: 0.0079\n",
      "Epoch [3001/10000]: Train loss: 0.0025\n",
      "Epoch [3501/10000]: Train loss: 0.0006\n",
      "Epoch [4001/10000]: Train loss: 0.0001\n",
      "Epoch [4501/10000]: Train loss: 0.0000\n",
      "Epoch [5001/10000]: Train loss: 0.0000\n",
      "Epoch [5501/10000]: Train loss: 0.0000\n",
      "\n",
      "Model is not improving, so we halt the training session.\n"
     ]
    }
   ],
   "source": [
    "x = [1, 2]\n",
    "ans_p = [0.1, 0.2, 0.3] # 自己假設的解答\n",
    "y = f(x, ans_p) \n",
    "pred_p = tuning(tune_model, config, device, x, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 實際的函數與model後的差距"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8 2.6]\n",
      "tensor([0.8000, 2.6000])\n",
      "tensor([0.8000, 2.6000])\n"
     ]
    }
   ],
   "source": [
    "print(f(x, ans_p))\n",
    "x, p = torch.FloatTensor(x).to(device), torch.FloatTensor(ans_p).to(device)\n",
    "print(proxy_model(x, p))\n",
    "print(tune_model.proxy_model(x, p))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 實際的參數與預測的參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "實際的參數:  [0.1, 0.2, 0.3]\n",
      "預測的參數:  [0.09638613 0.21450604 0.27457872]\n",
      "丟到函數裡後:\n",
      "[0.8 2.6]\n",
      "[0.79997694 2.60005058]\n"
     ]
    }
   ],
   "source": [
    "print(\"實際的參數: \", ans_p)\n",
    "print(\"預測的參數: \", pred_p)\n",
    "print(\"丟到函數裡後:\")\n",
    "print(f(x, ans_p))\n",
    "print(f(x, pred_p))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c263ad9d145de3501d3a40afec94bbb6fdefd8d26e88fb23f090a7b2e42b6a7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
