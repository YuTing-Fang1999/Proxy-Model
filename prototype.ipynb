{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Operations\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Reading/Writing Data\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# For Progress Bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Pytorch\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import qmc\n",
    "from itertools import product"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "config = {\n",
    "    'seed': 6302,      # Your seed number, you can pick your lucky number. :)\n",
    "    \"gen_num\": 100, \n",
    "    'valid_ratio': 0.2,   # validation_size = train_size * valid_ratio\n",
    "    'n_epochs': 6000,     # Number of epochs.            \n",
    "    'batch_size': 16, \n",
    "    'learning_rate': 1e-4,              \n",
    "    'early_stop': 800,    # If model has not improved for this many consecutive epochs, stop training.     \n",
    "    'save_path': './models/model.ckpt'  # Your model will be saved here.\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definition \n",
    "x: 原圖  \n",
    "p: 所下的參數  \n",
    "y: 處理過的圖 \n",
    "\n",
    "### $y = f(x, p)$\n",
    "假設x只有兩個pixel  \n",
    "$y_0 = p_0 * x_0^{2} + p_1 * x_1 + p_2$  \n",
    "$y_1 = p_0 * x_1^{2} + p_1 * x_0 + x_1$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, p): # x,p -> y\n",
    "    x = np.array(x).T\n",
    "    p = np.array(p).T\n",
    "    y0 = p[0]*x[0]**2 + p[1]*x[1] + p[2]\n",
    "    y1 = p[0]*x[1]**2 + p[1]*x[0] + x[1]\n",
    "    return np.array([y0, y1]).T\n",
    "\n",
    "# ex\n",
    "# x = [[1, 2]]\n",
    "# p = [[0.1, 0.2, 0.3], [0.2, 0.2, 0.3], [0.3, 0.2, 0.3]] # parameters are normalized to [0, 1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    '''\n",
    "    x: Features.\n",
    "    y: Targets, if none, do prediction.\n",
    "    '''\n",
    "    def __init__(self, x, y, p):\n",
    "        self.x = torch.FloatTensor(x)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "        self.p = torch.FloatTensor(p)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "            return self.x[idx], self.y[idx], self.p[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "def same_seed(seed): \n",
    "    '''Fixes random number generator seeds for reproducibility.'''\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def train_valid_split(data_set, valid_ratio, seed):\n",
    "    '''Split provided training data into training set and validation set'''\n",
    "    valid_set_size = int(valid_ratio * len(data_set)) \n",
    "    train_set_size = len(data_set) - valid_set_size\n",
    "    train_set, valid_set = random_split(data_set, [train_set_size, valid_set_size], generator=torch.Generator().manual_seed(seed))\n",
    "    return train_set, valid_set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Dataset\n",
    "* 初步實驗 假設都是同一張圖 (x固定)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[1, 2]]*config['gen_num'] # 假設圖固定，只修改參數\n",
    "sampler = qmc.LatinHypercube(d=3) # 假設p有3個參數\n",
    "p = sampler.random(n=config['gen_num']) # 隨機產生 gen_num 組\n",
    "\n",
    "dataset = MyDataset(x, f(x, p), p)\n",
    "train_dataset, valid_dataset = train_valid_split(dataset, config['valid_ratio'], config['seed'])\n",
    "\n",
    "# Pytorch data loader loads pytorch dataset into batches.\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Proxy_Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Proxy_Model, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1: Train Proxy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6000]: Train loss: 10.940766, Valid loss: 13.738888\n",
      "Epoch [101/6000]: Train loss: 8.572897, Valid loss: 11.356069\n",
      "Epoch [201/6000]: Train loss: 4.753461, Valid loss: 5.920917\n",
      "Epoch [301/6000]: Train loss: 1.232263, Valid loss: 1.862384\n",
      "Epoch [401/6000]: Train loss: 0.555594, Valid loss: 0.814832\n",
      "Epoch [501/6000]: Train loss: 0.475844, Valid loss: 0.529487\n",
      "Epoch [601/6000]: Train loss: 0.417189, Valid loss: 0.427874\n",
      "Epoch [701/6000]: Train loss: 0.357148, Valid loss: 0.341171\n",
      "Epoch [801/6000]: Train loss: 0.296983, Valid loss: 0.362515\n",
      "Epoch [901/6000]: Train loss: 0.237790, Valid loss: 0.232322\n",
      "Epoch [1001/6000]: Train loss: 0.181074, Valid loss: 0.165264\n",
      "Epoch [1101/6000]: Train loss: 0.128751, Valid loss: 0.162789\n",
      "Epoch [1201/6000]: Train loss: 0.083609, Valid loss: 0.101525\n",
      "Epoch [1301/6000]: Train loss: 0.049098, Valid loss: 0.050481\n",
      "Epoch [1401/6000]: Train loss: 0.026236, Valid loss: 0.019444\n",
      "Epoch [1501/6000]: Train loss: 0.012539, Valid loss: 0.015185\n",
      "Epoch [1601/6000]: Train loss: 0.005420, Valid loss: 0.005828\n",
      "Epoch [1701/6000]: Train loss: 0.002290, Valid loss: 0.001379\n",
      "Epoch [1801/6000]: Train loss: 0.001095, Valid loss: 0.000230\n",
      "Epoch [1901/6000]: Train loss: 0.000666, Valid loss: 0.000102\n",
      "Epoch [2001/6000]: Train loss: 0.000468, Valid loss: 0.000070\n",
      "Epoch [2101/6000]: Train loss: 0.000331, Valid loss: 0.000049\n",
      "Epoch [2201/6000]: Train loss: 0.000228, Valid loss: 0.000027\n",
      "Epoch [2301/6000]: Train loss: 0.000150, Valid loss: 0.000017\n",
      "Epoch [2401/6000]: Train loss: 0.000097, Valid loss: 0.000014\n",
      "Epoch [2501/6000]: Train loss: 0.000064, Valid loss: 0.000009\n",
      "Epoch [2601/6000]: Train loss: 0.000041, Valid loss: 0.000003\n",
      "Epoch [2701/6000]: Train loss: 0.000026, Valid loss: 0.000002\n",
      "Epoch [2801/6000]: Train loss: 0.000016, Valid loss: 0.000001\n",
      "Epoch [2901/6000]: Train loss: 0.000009, Valid loss: 0.000001\n",
      "Epoch [3001/6000]: Train loss: 0.000006, Valid loss: 0.000000\n",
      "Epoch [3101/6000]: Train loss: 0.000004, Valid loss: 0.000000\n",
      "Epoch [3201/6000]: Train loss: 0.000002, Valid loss: 0.000000\n",
      "Epoch [3301/6000]: Train loss: 0.000001, Valid loss: 0.000000\n",
      "Epoch [3401/6000]: Train loss: 0.000000, Valid loss: 0.000000\n",
      "Epoch [3501/6000]: Train loss: 0.000000, Valid loss: 0.000000\n",
      "Epoch [3601/6000]: Train loss: 0.000000, Valid loss: 0.000000\n",
      "\n",
      "Model is not improving, so we halt the training session.\n"
     ]
    }
   ],
   "source": [
    "def trainer(train_loader, valid_loader, model, config, device):\n",
    "\n",
    "    criterion = nn.MSELoss(reduction='mean') # Define your loss function, do not modify this.\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate']) \n",
    "\n",
    "    if not os.path.isdir('./models'):\n",
    "        os.mkdir('./models') # Create directory of saving models.\n",
    "\n",
    "    n_epochs, best_loss, early_stop_count = config['n_epochs'], math.inf, 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train() # Set your model to train mode.\n",
    "        loss_record = []\n",
    "\n",
    "        for x, y, p in train_loader:\n",
    "            optimizer.zero_grad()               # Set gradient to zero.\n",
    "            # 假設都是同一張圖(只輸入參數 p )\n",
    "            x, y = p.to(device), y.to(device)   # Move your data to device. \n",
    "            pred = model(x)             \n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()                     # Compute gradient(backpropagation).\n",
    "            optimizer.step()                    # Update parameters.\n",
    "            loss_record.append(loss.detach().item())\n",
    "\n",
    "        mean_train_loss = np.round(sum(loss_record)/len(loss_record), 6)\n",
    "\n",
    "        model.eval() # Set your model to evaluation mode.\n",
    "        loss_record = []\n",
    "        for x, y, p in valid_loader:\n",
    "            x, y = p.to(device), y.to(device)\n",
    "            with torch.no_grad():\n",
    "                pred = model(x)\n",
    "                loss = criterion(pred, y)\n",
    "\n",
    "            loss_record.append(loss.item())\n",
    "            \n",
    "        mean_valid_loss = np.round(sum(loss_record)/len(loss_record), 6)\n",
    "        if epoch%100==0:\n",
    "            print(f'Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.6f}, Valid loss: {mean_valid_loss:.6f}')\n",
    "\n",
    "        if mean_valid_loss < best_loss:\n",
    "            best_loss = mean_valid_loss\n",
    "            torch.save(model.state_dict(), config['save_path']) # Save your best model\n",
    "            # print('Saving model with loss {:.3f}...'.format(best_loss))\n",
    "            early_stop_count = 0\n",
    "        else: \n",
    "            early_stop_count += 1\n",
    "\n",
    "        if early_stop_count >= config['early_stop']:\n",
    "            print('\\nModel is not improving, so we halt the training session.')\n",
    "            return\n",
    "        \n",
    "proxy_model = Proxy_Model(input_dim=3, output_dim=2).to(device) # put your model and data on the same computation device.\n",
    "trainer(train_loader, valid_loader, proxy_model, config, device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step2: Fix Proxy Model and Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tune_Proxy_Model(nn.Module):\n",
    "    def __init__(self, input_dim, proxy_model):\n",
    "        super(Tune_Proxy_Model, self).__init__()\n",
    "        self.proxy_model = proxy_model\n",
    "        self.tuning_fn = nn.Linear(input_dim, input_dim, bias=False)\n",
    "\n",
    "    def forward(self, p):\n",
    "        p = self.tuning_fn(p)\n",
    "        y = self.proxy_model(p)\n",
    "        return y\n",
    "    \n",
    "# Load Proxy Model\n",
    "proxy_model = Proxy_Model(input_dim=3, output_dim=2).to(device)\n",
    "proxy_model.load_state_dict(torch.load(config['save_path']))\n",
    "\n",
    "tune_model = Tune_Proxy_Model(3, proxy_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning(model, config, device, x, y):\n",
    "\n",
    "    criterion = nn.MSELoss(reduction='mean') # Define your loss function, do not modify this.\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate']) \n",
    "\n",
    "    n_epochs, best_loss, early_stop_count = 10000, math.inf, 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.tuning_fn.train()\n",
    "        model.proxy_model.eval() \n",
    "        # fix proxy model\n",
    "        for param in model.proxy_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        loss_record = []\n",
    "\n",
    "        optimizer.zero_grad()               # Set gradient to zero.\n",
    "        init_p = [1, 1, 1] # init p\n",
    "        init_p, y = torch.FloatTensor(init_p).to(device), torch.FloatTensor(y).to(device)   # Move your data to device. \n",
    "        pred = model(init_p)             \n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()                     # Compute gradient(backpropagation).\n",
    "        optimizer.step()                    # Update parameters.\n",
    "        model.tuning_fn.weight.data.clamp_(min=0, max=1)\n",
    "        loss_record.append(loss.detach().item())\n",
    "\n",
    "        mean_train_loss = np.round(sum(loss_record)/len(loss_record), 6)\n",
    "        \n",
    "        if epoch%500==0:\n",
    "            print(f'Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.4f}')\n",
    "\n",
    "        if mean_train_loss < best_loss:\n",
    "            best_loss = mean_train_loss\n",
    "            early_stop_count = 0\n",
    "        else: \n",
    "            early_stop_count += 1\n",
    "\n",
    "        \n",
    "\n",
    "        if early_stop_count >= config['early_stop']:\n",
    "            print('\\nModel is not improving, so we halt the training session.')\n",
    "            break\n",
    "        \n",
    "    return model.tuning_fn(init_p).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10000]: Train loss: 1.1900\n",
      "Epoch [501/10000]: Train loss: 0.8884\n",
      "Epoch [1001/10000]: Train loss: 0.5986\n",
      "Epoch [1501/10000]: Train loss: 0.3858\n",
      "Epoch [2001/10000]: Train loss: 0.2360\n",
      "Epoch [2501/10000]: Train loss: 0.1349\n",
      "Epoch [3001/10000]: Train loss: 0.0760\n",
      "Epoch [3501/10000]: Train loss: 0.0449\n",
      "Epoch [4001/10000]: Train loss: 0.0269\n",
      "Epoch [4501/10000]: Train loss: 0.0146\n",
      "Epoch [5001/10000]: Train loss: 0.0068\n",
      "Epoch [5501/10000]: Train loss: 0.0026\n",
      "Epoch [6001/10000]: Train loss: 0.0007\n",
      "Epoch [6501/10000]: Train loss: 0.0002\n",
      "Epoch [7001/10000]: Train loss: 0.0000\n",
      "Epoch [7501/10000]: Train loss: 0.0000\n",
      "Epoch [8001/10000]: Train loss: 0.0000\n",
      "\n",
      "Model is not improving, so we halt the training session.\n"
     ]
    }
   ],
   "source": [
    "x = [1, 2]\n",
    "ans_p = [0.1, 0.2, 0.3] # 自己假設的解答\n",
    "y = f(x, ans_p)\n",
    "pred_p = tuning(tune_model, config, device, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8 2.6]\n",
      "tensor([0.7980, 2.5983])\n",
      "tensor([0.7980, 2.5983])\n"
     ]
    }
   ],
   "source": [
    "print(f(x, ans_p))\n",
    "print(proxy_model(torch.FloatTensor(ans_p).to(device)))\n",
    "print(tune_model.proxy_model(torch.FloatTensor(ans_p).to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "實際的參數:  [0.1, 0.2, 0.3]\n",
      "預測的參數:  [5.7339117e-02 3.7240133e-01 4.2605243e-05]\n",
      "[0.8 2.6]\n",
      "[0.80218438 2.60175779]\n"
     ]
    }
   ],
   "source": [
    "print(\"實際的參數: \", ans_p)\n",
    "print(\"預測的參數: \", pred_p)\n",
    "print(f(x, ans_p))\n",
    "print(f(x, pred_p))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c263ad9d145de3501d3a40afec94bbb6fdefd8d26e88fb23f090a7b2e42b6a7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
